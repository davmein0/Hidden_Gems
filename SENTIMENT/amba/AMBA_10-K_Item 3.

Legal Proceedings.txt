Item 3.

Legal Proceedings

44

Item 4.

Mine Safety Disclosures

44

PART II

Item 5.

Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities

45

Item 6.

[Reserved]

47

Item 7.

Management’s Discussion and Analysis of Financial Condition and Results of Operations

48

Item 7A.

Quantitative and Qualitative Disclosures About Market Risk

59

Item 8.

Financial Statements and Supplementary Data

61

Item 9.

Changes in and Disagreements With Accountants on Accounting and Financial Disclosure

62

Item 9A.

Controls and Procedures

62

Item 9B.

Other Information

63

Item 9C.

Disclosure Regarding Foreign Jurisdictions that Prevent Inspections

63

PART III

Item 10.

Directors, Executive Officers and Corporate Governance

64

Item 11.

Executive Compensation

64

Item 12.

Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters

64

Item 13.

Certain Relationships and Related Transactions, and Director Independence

64

Item 14.

Principal Accountant Fees and Services

64

PART IV

Item 15.

Exhibits and Financial Statement Schedules

65

Item 16.

Form 10-K Summary

94

Exhibits

94

Signatures

97

Power of Attorney

97

2

FORWARD-LOOKING STATEMENTS

This Annual Report on Form 10-K contains forward-looking statements within the meaning of Section 27A of the Securities Act and Section 21E of the Exchange Act. The forward-looking statements are contained principally in, but not limited to, the sections titled “Business,” “Risk Factors,” and “Management’s Discussion and Analysis of Financial Condition and Results of Operations,” as well as elsewhere in this Annual Report on Form 10-K. Forward-looking statements are identified by the use of the words “would,” “could,” “will,” “may,” “expect,” “believe,” “should,” “anticipate,” “outlook,” “if,” “future,” “intend,” “plan,” “estimate,” “predict,” “potential,” “target,” “seek,” “continue,” “foreseeable” or “forecast” and similar words and phrases, including the negatives of these terms, or other variations of these terms, that denote future events. Forward-looking statements include, but are not limited to, information concerning our possible or assumed future results of operations, competitive position, industry environment, potential growth opportunities and the effects of competition, our product development strategy and areas of focus, our market opportunity, our ability to develop new solutions, including our ability to integrate and apply acquired technologies to our solutions, our future financial and operating performance, sales and marketing strategy, investment strategy and the results of our investments, research and development, customer and supplier relationships, inventory levels, customer demand and our ability to secure design wins, industry trends, our cash needs and capital requirements, and expectations about seasonality, taxes, and operating expenses. These statements reflect our current views with respect to future events and our potential financial performance and are subject to risks and uncertainties that could cause our actual results and financial position to differ materially and adversely from what is projected or implied in any forward-looking statements included in this Annual Report on Form 10-K.

Factors that could affect such forward-looking statements include, but are not limited to, risks associated with revenue being generated from new customers or design wins, neither of which is assured; our ability to retain and expand customer relationships and to achieve design wins; economic factors beyond our control, including risks associated with high inflation and recessionary concerns; geopolitical factors beyond our control, including tensions between the United States and China and the ongoing hostility between Russia and Ukraine; the potential impact of pandemics and endemics on our operations or the operations of our supply chain or our customers; our ability to timely produce sufficient quantities of our products on a cost-effective basis through our third-party vendors; the commercial success of our customers’ products; our growth strategy; our ability to anticipate future market demands and future needs and preferences of our customers; our ability to introduce new and enhanced solutions, including our ability to license software modules; the expansion of our current markets and our ability to successfully enter new markets; anticipated trends and challenges, including competition, in the markets in which we operate; our expectations regarding the adoption of computer vision technology; our ability to effectively generate and manage growth; our ability to retain key employees; the potential for intellectual property disputes or other litigation; the risks described under Item 1A of Part I—“Risk Factors,” Item 7 of Part II—“Management’s Discussion and Analysis of Financial Condition and Results of Operations,” and elsewhere in this Annual Report on Form 10-K; and those discussed in other documents we file with the Securities and Exchange Commission. You are cautioned not to place undue reliance on the forward-looking statements, which speak only as of the date of this Annual Report on Form 10-K. We have no obligation (and expressly disclaim any such obligation) to update or alter any forward-looking statements, whether as a result of new information or otherwise except as otherwise required by securities regulations.

For purposes of this Annual Report, the terms “Ambarella”, “the Company”, “we”, “us” and “our” refer to Ambarella, Inc. and its consolidated subsidiaries.

3

P
ART I

ITEM 1. B
USINESS

Overview

Incorporated in 2004, Ambarella is a leading developer of low-power system-on-a-chip, or SoC, semiconductors and software for edge artificial intelligence, or AI, applications. Our technologies make electronic systems smarter, enabling them to become partially or fully autonomous with features such as person detection, object classification, and analytics, in addition to performing complex data analysis in real time, delivering high quality imagery, and preserving vital system resources such as power and network bandwidth. We specialize in the development of deployable, scalable designs for intelligent electronic systems that utilize high-bandwidth sensors offering a proven path to mass production. Ambarella’s products are used in a wide variety of human viewing, computer vision and edge AI applications, including a variety of automotive camera systems, video security cameras, mobile and fixed robots, industrial applications, and consumer devices, such as action, drone and 360° cameras.

Until 2023, a majority of our revenue originated from human-viewing only applications with video and image processors for enterprise, public infrastructure and home applications, such as internet protocol, or IP, security cameras, sports cameras, wearables, aerial drones, and aftermarket automotive video recorders. Since 2018, we have been leveraging our human-viewing heritage to pursue the machine sensing market. Our recent development efforts have focused on creating advanced AI inference technology that enables edge devices to perceive the environment and make decisions based on the data collected from cameras and other types of sensors, such as 4D radar. This is known as edge AI, and our AI inference SoCs integrate our state-of-the-art video processor technology together with our proprietary AI accelerator, also known as a deep learning neural network processor, which we refer to as CVflow®. The CVflow-architecture supports a variety of AI inference algorithms, including object detection, classification and tracking, semantic and instance segmentation, image processing, stereo object detection, and terrain mapping. Our latest third generation CVflow technology enables us to efficiently process transformer AI networks, which facilitate incremental and computationally intense advanced AI applications, including deep fusion, deep planning, large language models (LLMs) and reasoning models. In addition, CVflow can process data from other sensing modalities, including lidar, radar, time of flight, thermal and near infrared (NIR), and allows customers to differentiate their products by porting their own or third party neural networks and/or AI algorithms to our CVflow-based SoCs. Our AI technology is creating opportunities for us to address a broader range of markets and applications while also allowing us to capture more content per electronic system.

Our newest product families, CV7, CV3 and N1 integrate our 3
rd
generation CVflow AI accelerator and are all built on advanced 5 nanometer (nm) process technology. These products can run some of the latest transformer neural networks which can provide incremental efficiency and utility relative to convolutional neural networks. The CV7 family provides an industry-leading combination of low power and high performance in human vision, computer vision and advanced AI applications, including both automotive and Internet-of-Things (IoT) edge AI applications.

Our CV3 AI central domain controller family of SoCs is specifically architected for partially to fully automated driving applications.  In addition to offering our existing advanced camera perception processing, CV3 adds sensor fusion and planning layers that enable a broader set of fully-automated devices.

Our N1 SoC is capable of running LLM inferencing with models up to 34 billion parameters, enabling a range of AI applications in IoT devices, including industrial robotics, intelligent healthcare imaging and diagnostics, edge AI servers running multi-modal vision-language models (VLMs) and traditional LLMs, and autonomous fleet telematics.

Our Oculii adaptive AI software algorithms are designed to enable radar perception using current production radar chips to achieve significantly higher resolution, longer range and greater accuracy. We have introduced a centralized radar architecture that synergistically leverages these adaptive AI software algorithms together with our CV3 domain controller family, resulting in improved perception, lower power consumption and reduced bills-of-material for mobility applications compared to the current generation of radar systems utilized in the market today.

Industry Background and Target Markets

AI functionality has historically been executed with graphics processing units (GPU), field programmable gate-arrays (FPGA) or general purpose microprocessors (CPU) in servers or data centers. This approach requires large amounts of data to be transported from an end-point electronic system or device into the network infrastructure, where the data may be stored, processed, and then sent back to the end point, creating added delay, power consumption and incremental expense from data communications, server processing and storage. In some applications, unacceptable levels of latency are introduced by the transportation of this data, minimizing or, in some cases, eliminating the utility of the product. In addition, this approach often requires personal information to be transmitted from the end-point device to the network infrastructure, potentially raising privacy and security concerns.

4

We believe the AI inference end-point market, sometimes referred to as the system’s edge, requires a fundamentally different SoC architecture versus the GPU, FPGA and CPU approach commonly used in the data center.  Our AI SoCs are optimized for the requirements of the edge inference market to provide highly accurate results, significant processing power, small form factor and minimal latency while consuming very low amounts of power and simultaneously delivering both human viewing and AI inference

functionality, often while supporting multiple cameras and multiple applications with a single SoC incorporated in an end-point device.  In addition, privacy and security can be enhanced, as critical personal information may not need to enter the network infrastructure.

Our first AI SoC was introduced in 2018 and the CV3 SoC integrates our third generation AI accelerator, which we refer to as CVflow. Our development efforts are now focused on SoCs that provide both human viewing and AI inference functionality. With the acquisition of Oculii, we complement our advanced camera perception capabilities with advanced radar perception to enable higher levels of autonomy.

We are focusing on the automotive and IoT end markets that require increasingly sophisticated AI inference workloads and processing performance:

Automotive Applications:

Cameras and other sensors, as well as high performance computing processors, are utilized for a variety of applications in the automotive market and our products are designed into both original equipment manufacturer (OEM) and aftermarket applications. We address both the safety (e.g., advanced driver assistance systems (“ADAS") and autonomy automotive market applications:

Safety and ADAS:

▪
Front Advanced Driver Assistance System (ADAS) Cameras.
 These front-facing cameras are often positioned behind the rearview mirror, enabling functions such as automatic emergency braking, lane departure warning, forward collision warning, intelligent headlight control, and speed assistance functions, many of which are required by an increasing number of regional New Car Assessment Programs, or NCAP.  These cameras may also be used by telematic service providers to help improve safety, lower insurance costs, and improve driver performance.

▪
Cabin Monitoring System (CMS) and Driver Monitoring System (DMS) Cameras.
 These interior mounted cameras track drivers and passengers to help prevent accidents by alerting a drowsy or distracted driver and assisting with the deployment of safety features, such as airbags. These interior cameras may also be utilized by telematic service providers to provide feedback on driver performance and behavior.

5

▪
Electronic Mirrors.
 One or more cameras, in conjunction with an electronic display, are used to augment, or in some cases replace, reflective glass rear view and/or side view mirrors to provide a wider and unobstructed field of view.  Smart electronic mirrors that incorporate our AI SoCs may also help with detecting objects in blind spots, overtaking vehicles and alerting for vulnerable road users, such as pedestrians and bicycles. 

▪
Automotive Video Recorders (also known as data loggers).
 These video cameras are pre-installed in vehicles or mounted (aftermarket) to record events for reconciliation, such as for insurance and liability, driver scoring or training, and security purposes. We offer solutions for both OEM and aftermarket drive recording devices, some of which include ADAS features.

Autonomy:

•
Central domain controllers for L2+ to L4 Autonomous Vehicles. 
We continue to advance our research in critical areas of autonomous vehicle development, such as vehicle detection, obstacle detection, pedestrian detection, lane detection, traffic sign recognition, stereovision processing, and sensor fusion and planning, enabling us to design strong platforms for applications ranging from Level2+ autopilot to full autonomy. The CV3 family enables centralized, single-chip processing for multi-sensor perception, including high-resolution vision, radar, ultrasonic and lidar, as well as deep fusion for multiple sensor modalities and autonomous vehicle path planning.  In addition, the domain controller can simultaneously process in-cabin sensing applications, including driver and occupant monitoring.

IoT Applications:

•
Security Cameras
.  
We are a leader in enterprise and home security camera markets, with solutions that deliver exceptional AI inference performance, industry-leading compression efficiency, low power consumption, and outstanding image quality, including high dynamic range (HDR), low-light processing and fisheye lens de-warping. Our AI products enable higher levels of automation than our vision processors through advanced algorithms, such as object detection, classification and tracking, license plate recognition and facial recognition.  We address the following security camera applications:

▪
Enterprise and Public Class Security
. These cameras are used for video monitoring and security surveillance in enterprise and public infrastructure applications. Embedded advanced AI technology supports advanced analytics at the system’s edge, including people counting and tracking, facial recognition and retail behavior analysis. In addition, our latest GenAI SoC, the N1-655, can run the latest AI models, without the need for an internet connection, on a variety of on-premise devices, including smart-city security video recorders.

▪
Home Security
. Home security cameras are designed for home or small business use and may be connected to cloud services and applications via home networks using WiFi. Form factors include smart video door-bells and video-enabled lights. Embedded AI technology supports advanced functions, including intruder and pet detection, face recognition and package monitoring.  

•
Emerging Robotic and Industrial Applications
.   Our solutions can add intelligence to a range of partially or fully robotic applications, including access control, industrial/factory automation, sensing cameras, and a variety of industrial and home robotic applications. Our advanced AI SoCs handle an array of complex algorithms, from low-level perception functions and neural networks to higher-level autonomous software stacks, while our neural network-based image signal processing (NN-ISP) provides advanced noise reduction to enable better performance in challenging lighting conditions such as high-contrast scenes and extremely low-light environments, all with low power consumption.  We address the following industrial and robotic market applications:

▪
Identification/Authentication Cameras.
  Our video-based sensing solutions enable contactless access control for home, enterprise and public applications. Applications include enterprise access control panels, electronic locks and contactless mobile payment terminals.  

▪
Robotic Products.  
Our products and technology are well suited for a variety of smart home and enterprise robotic applications.  With stereovision capabilities, convolutional neural network (CNN)-based object classification and transformer neural networks, our solutions are also suited for a variety of industrial machine vision systems, mobile robots for delivery or factory/warehouse applications, aerial drones, robotic vacuum cleaners, and other emerging robotic applications.

▪
Sensing Cameras.

Our AI SoCs enable sensing cameras that analyze video using AI-based algorithms running in the camera to provide remote users with updates, warnings or business data based on the analysis. Since no video, audio or image data needs to leave the camera, privacy can be prioritized. Applications for sensing cameras include elderly monitoring, building occupancy monitoring and retail store business analytics.

6

•
Other IoT Applications.   
Cameras for the enterprise, home, public spaces and consumer leisure applications that provide high-definition (HD) video quality increasingly include embedded connectivity to share and display video. Our low power, high-resolution and connected solutions can be found in a variety of cameras, including wearable body cameras, sports action cameras, social media cameras, drones for capturing aerial video or photographs, video conferencing and virtual reality applications.

Our Competitive Strengths

Our platform technology solutions provide performance attributes that satisfy the stringent demands of the camera market, enable integration of HD video and image capture capabilities in portable devices, and provide AI inference capabilities that address the evolving needs of the automotive and IoT markets. We believe that our leadership is the result of our competitive strengths, including:

•
Proprietary AI, Radar and Computer Vision Architecture.

Our proprietary AI processing architecture, known as CVflow, uses a flexible hardware engine programmed with a high level algorithm description to achieve increased performance while minimizing die size and power consumption. The CVflow architecture supports a variety of AI, radar and computer vision algorithms, including object detection, classification and tracking, semantic and instance segmentation, image processing, and stereo object detection. Our third generation CVflow technology enables us to efficiently process transformer AI networks, which are an enabling technology for next generation automotive and generative AI markets. CVflow also allows customers to differentiate their products by porting their own algorithms and neural networks to our SoCs.  

•
Deep Sensor Fusion
. Ambarella provides AI perception processing for cameras and software that enables efficient HD 4D radar perception. Our CV3 SoC family implements centralized camera and radar perception processing on the same SoC, allowing data from all camera and radar sensors in the sensor suite to be fused at a deeper data level, which we believe will facilitate improved levels of perception accuracy.

•
High-Performance, Low Power, AI, Video and Image Algorithm Expertise
. Our extensive algorithm expertise, which facilitates efficient AI, video and image compression, enables our solutions to achieve low power consumption without compromising performance. Our solutions support high resolution, high frame-rate and multi-stream video capabilities. Our solutions achieve high storage and transmission efficiencies through innovative and complex video and image compression algorithms that significantly reduce the output bitrate, which directly benefits the performance of our solutions in several ways, including lower memory storage requirements, reduced bandwidth needs for transmission and lower cloud storage costs.  Our solutions can deliver clear images in low light conditions because of our advanced noise reduction, including 3D motion compensated temporal filtering (MCTF) and multiple exposure processing. Additionally, our HDR processing capabilities handle scenes with large dynamic range between the lightest and darkest areas to reveal details that would otherwise be lost in shadow or highlight areas. Our neural network-based image signal processing (NN-ISP) provides advanced noise reduction in extremely low lighting conditions. Our advanced de-warping capability enables cameras to use wide angle lenses, making it ideal for a variety of security camera applications, as well as 3D electronic image stabilization and surround view for automotive applications.

•
Highly Integrated SoC Solutions Based on a Scalable Platform
. Our product families leverage a flexible and highly-scalable platform including our core high-performance AI and video processing architecture combined with an extensive set of integrated peripherals.  Our flexible and highly-scalable platform enables our customers to address multiple applications and markets with reduced design cycles and costs. Our software compatible portfolio of products, with a broad range of performance and price points, allows our customers to develop a wide range of differentiated end products from a common software base. 

•
Comprehensive and Flexible Software
. Our years of investment in developing and optimizing our comprehensive and flexible software serve as the foundation of our high-performance video application solutions. We provide our customers full-function software development kits with a suite of application programming interfaces or APIs, which allow customers to rapidly integrate our solution, adjust product specifications and provide additional functionality to their systems, thereby enabling them to differentiate their product offerings and reduce time to market. We also provide a toolkit to accelerate the development of computer vision algorithms onto our hardware. 
Our Cooper™ Developer Platform offers seamless integration of software, hardware, state-of-the-art (SOTA) AI models, and services that provide universal support for Ambarella’s entire portfolio of AI SoCs. 

7

Products

We have a wide range of products in our portfolio, including products that have commercially shipped, products for which we have shipped engineering samples and products that are under development. We typically introduce two to three new silicon products per year which, when combined with our flexible software development kits, allow us to offer product families addressing the specific needs of a wide range of end markets. In addition to enabling small device size and low power consumption, our SoC solutions make possible differentiated functionalities, such as advanced AI functionality, simultaneous video and image capture, multiple-stream video capture, image stabilization and wireless connectivity.

Central Domain Controller.
 Our CV3-AD family of automotive AI domain controllers, targets L2+ to L4 autonomous vehicles and advanced robots. Its next-generation CVflow® AI accelerator includes neural network processing that is 20x faster than the previous generation of CV2 SoCs, along with additional general vector processing capabilities to provide the overall performance required for full autonomous driving (AD) stack processing, including computer vision, HD 4D radar, deep fusion and planning. It also integrates advanced image processing, a dense stereo and optical flow engine, Arm® Cortex® A78AE and R52 CPUs, an automotive GPU for visualizations, and a hardware security module (HSM). The CV3-AD685 is an “algorithm first” architecture that provides support for the entire AD software stack.

CVflow SoCs.
 Multiple generations of our AI accelerator architecture have been developed, and all of our new product families incorporate the ability to extract and processes data from video streams, enabling our customers to develop intelligent camera systems. These SoCs combine advanced image processing, high-resolution video encoding and CVflow AI processing in a single, low-power design to enable a new class of smart edge devices for applications including smart home security, retail monitoring, consumer robotics, and occupancy monitoring. Some of our CVflow SoCs are manufactured to satisfy the functional safety requirements of the automotive market. Our third generation CVflow-based SoCs enable efficient processing or transformer AI networks, which are an enabling technology for next generation automotive and generative AI markets.

AI Neural Processor. 
Based on our proprietary architecture, our N1 SoC provides highly-efficient AI performance for neural network computation in combination with a general vector processor (GVP), an advanced image processor, a dense stereo and optical flow engine, and a GPU, in a single SoC.  The N1 is designed for implementing industrial robotics, smart cities, intelligent healthcare imaging and diagnostics, multi-camera AI processing hub, edge AI servers running multi-modal LLMs, and autonomous fleet telematics.

Vision Processor SoCs.
 Our video and image processing SoCs integrate an advanced image sensor pipeline (ISP), H.264 and/or H.265 encoders, and a powerful ARM CPU for advanced analytics, flight control, WiFi streaming, and other user applications.  Our unique architecture and advanced process node technology lower power consumption while maintaining high performance for security camera and consumer applications such as connected drones, sports cameras, and 360º (VR) cameras.

High Definition Radar
.
  Through our acquisition of Oculii, we offer adaptive AI software algorithms designed to enable radar perception using current production radar chips to achieve significantly higher resolution, longer range and greater accuracy. These improvements eliminate the need for specialized high-resolution radar chips, which have significantly higher power consumption and cost than conventional radar solutions. We recently introduced a centralized radar architecture that leverages Oculii’s adaptive AI software algorithms together with our CV3 processor family to enable both central processing of raw radar data and deep, low-level fusion with other sensor inputs, including cameras, lidar and ultrasonics.

Serializer/Deserializers
.
  Our B8 SerDes (Serializer/Deserializer) product is a mixed-signal (analog and digital) semiconductor used to transport data short distances (up to 10 meters) from a CMOS image sensor, often in a remote camera location, to our video and AI SoCs.  The SerDes chips are used to add additional camera(s) to an automotive application, as well as used as a bridge chip for other automotive applications, such as a MIPI combiner, splitter or display driver.  Our SerDes chips are also used in security applications such as ATMs that can use a single B8 chip for connecting multiple remote cameras to a single video processor SoC.

Software Modules
.
  We separately license proprietary software modules that can be used in conjunction with a customer’s internally developed software and/or with third-party software.  Features that may be licensed include functionality for a variety of automotive applications, including dataloggers, ADAS and autonomous driving systems, eMirrors and in-cabin applications. Additionally, our neural-network image signal processing (NN-ISP) software module improves low light imaging in security camera applications.

8

The chart below describes our current product lines:

Technology

Our semiconductor processing solutions enable edge AI processing, HD, Ultra HD (UHD) and 8K UHD (up to 7680 x 4320p60) video and image processing, and video compression, sharing and display while offering exceptional power, size, and performance characteristics.

Key differentiators of our technology include:

•
flexible and scalable CVflow processors for deep learning, HD radar processing and other AI algorithms that cover a broad range of consumer, professional and automotive requirements with power and die size efficiency; 

•
stereo/optical flow processing engines for robust AI processing with high performance and power efficiency;

•
scalable image processing and video compression engines that cover consumer, professional and automotive requirements from Full HD to 8K video performance levels as well as multiple image sensors simultaneously to support multiple viewpoints, including surround view and virtual reality applications; 

9

•
algorithms for image processing including deep learning augmented processing for challenging low light and high dynamic range conditions for robust AI and human viewing with high power efficiency. 

•
algorithms and software for scalable and robust HD 4D radar processing using sparse antenna arrays using machine learning and adaptive transmit waveforms for lower cost and better power efficiency;

•
deep learning algorithms and software for multi class 2D/3D object detection and segmentation, including vehicles, pedestrians, cycles, road markings, traffic signs and traffic lights optimized for our CV2 and CV3 SoC families;

•
algorithms and software for stereo obstacle detection to provide robust safety in the event of obstacles that are not in the training data; 

•
autonomous driving stack modules optimized for our CV3 SoC family, including fusion for multiple cameras and sensor modalities, mapping and localization algorithms and planning;

•
algorithms to compress video signals with high compression and power efficiency at multiple operating points; 

•
software development kit comprised of application programming interfaces, or APIs, to facilitate integration into customers’ products; and tools for porting and optimizing customer deep neural networks, or DNNs, developed in industry standard training frameworks; 

•
low-power architecture with minimal system memory footprint; and

•
programmable architecture that balances flexibility, quality, power and die size with powerful CPUs and optimized hardware acceleration to support advanced processing functions.

Our technology platform is based on a high-performance, low-power architecture supported by a high level of system integration. The building blocks of our platform are illustrated below:

Our technology platform enables the capture of high-resolution still images and UHD video while simultaneously performing AI processing and encoding for high-quality storage and lower resolution real time streaming.

CVflow

10

Our proprietary AI processing architecture, known as CVflow®, uses a flexible hardware accelerator programmed with a data flow graph algorithm description to achieve increased performance while minimizing die size and power consumption. This description allows the hardware to maximize use of its resources by exploiting all available parallelism without software intervention.  The CVflow architecture specifies data flow connections between a set of optimized AI and computer vision operators, such as the convolution and matrix multiply functions that are used for deep learning algorithms. Our CVflow engine is also capable of running large language model inferencing, with models up to 34 billion parameters run on a single N-1 SoC. The CVflow architecture also supports a variety of other algorithms, including radar processing, stereo obstacle detection and sensor fusion.  Our third generation CVflow-based SoCs enable efficient processing or transformer AI networks, which are an enabling technology for next generation automotive and generative AI markets. Our platform allows customers to differentiate their products by porting and optimizing their own algorithms and neural networks to our CVflow-based chips using industry-standard training tools and APIs.

Computer Vision and Radar Technology

Computer vision is a core technology that complements our proprietary image processing and video compression technology. We have developed efficient deep learning algorithms for object detection and segmentation leveraging our deep understanding of the CVflow processor. A significant feature of our third generation CVflow SoCs is support for HD stereo and HD radar based depth and velocity sensing. We believe HD stereo and HD radar are complementary sensor modalities that provide robust depth information after fusion. This depth information provides an important augmentation to monocular computer vision processing, resulting in an extra margin of safety for autonomous driving and other applications. Monocular processing depends on training to detect obstacles, and may not detect obstacles that are not represented in the training set. Stereo cameras and radar detect obstacles without relying on training for specific obstacle categories because the depth information is used to directly construct a three-dimensional model of the camera’s surroundings, including any obstacles. This allows more robust decisions to be made in applications such as autonomous driving.

Software Modules

We are developing optimized software modules to give customers the option to leverage our expertise and reduce development time and expense.  These modules include HD radar processing for standalone and central radar processing, deep learning based low light and HDR image processing, monocular and stereo camera perception, and autonomous driving stack modules optimized for the CV3 family, including fusion for multiple cameras and sensor modalities, mapping and localization algorithms and planning.

AmbaClear

Our proprietary image signal processing architecture, known as AmbaClear, incorporates advanced algorithms to convert raw sensor data to UHD video and/or still images. Image processing algorithms include sensor, lens and color correction, HDR tone mapping, color processing and de-mosaicing to reconstruct a full color image from incomplete color samples and specialized color filters, noise filtering, detail enhancement and image format conversion. For example, raw sensor data can be captured at up to 32-megapixel (8K) resolution at 60 frames per second. This image processing reduces noise in the sensor data and improves color, contrast and sharpness resulting in improved computer vision performance, enhanced human viewing and enhanced storage and transmission efficiencies. Our wide dynamic range (WDR) and HDR processing capabilities handle greater dynamic range between the lightest and darkest areas of an image, permitting video images to reveal details that would otherwise be lost against a bright background. We have developed efficient scalable deep learning algorithms for advanced low light processing and HDR tone mapping that augment our image processing hardware. These algorithms provide significant image quality improvements over our standard image processing while running in real time at HD and higher resolutions. Our advanced de-warping capability enables cameras to use wide angle lenses to capture images from a wide area, making it ideal for a variety of IP security camera and surround view applications. Our RGB- infrared fusion capability allows a single sensor to produce simultaneous RGB and infrared images for sensing and improved low light performance.

AmbaCast

Our proprietary UHD video compression architecture, known as AmbaCast, incorporates advanced algorithms for motion estimation, motion-compensated 3D temporal filtering, mode decision and AI based rate control. Successful implementation of these computationally intensive steps has helped us maximize compression efficiency. We support H.264 and H.265 video compression standard with our H.265 providing up to 2x better compression efficiency compared to our H.264 video compression technology.

11

Design Methodology

         The success of our technology platform stems from our algorithm driven design methodology. We do extensive algorithm studies in deep learning AI, image processing and compression including our internally developed and public external algorithms. We use these studies to develop high power and die area efficient processing engines compared with general purpose processors like CPUs and GPUs. We also include a high degree of programmability to provide flexibility in supporting new algorithms that we and our customers develop. We test and verify our algorithms on our proprietary architectural model prior to implementing our processor engines in hardware. Our advanced verification methodology validates our approach through simultaneous modeling of architecture, algorithms, and the hardware itself. This redundant approach helps us identify and remediate weaknesses early in the development cycle, providing a solid foundation on which we build our hardware implementation, and enhances our ability to achieve first-pass silicon success. We possess extensive expertise in AI deep learning, video and imaging algorithms, as well as deep sub-micron digital and mixed-signal design experience.

Customers

We sell our solutions to leading original design manufacturers, or ODMs, and original equipment manufacturers, or OEMs, globally. In the automotive OEM market, we may sell our solutions to Tier-1 suppliers that develop and sell devices incorporating our solutions to automotive OEMs. We refer to ODMs and Tier-1 suppliers as our customers and OEMs as our end customers, except as otherwise indicated or as the context otherwise requires.

Sales to customers in Asia accounted for approximately 85%, 79%, and 79% of our total revenue in the fiscal years ended January 31, 2025, 2024, and 2023, respectively. As many of our OEM end customers or their ODM manufacturers are located in Asia, we anticipate that a majority of our revenue will continue to come from sales to customers in that region. Although a large percentage of our sales are made to customers in Asia, we believe that a significant number of the products designed by these customers and incorporating our SoCs are then sold to consumers globally. To date, all of our sales have been denominated in U.S. dollars.

We work closely with our end customer OEMs and ODMs throughout their product design cycles that often last twelve to eighteen months for many of our target markets, although new products may have longer design cycles, particularly those implementing advanced AI features.  Product design cycles for certain portions of the automotive market generally last longer than eighteen months, particularly for products containing user safety features. As a result, we are able to develop long-term relationships with our customers as our technology becomes embedded in their products. Consequently, we believe we are well positioned to not only be designed into our customers’ current products, but also to continue to develop next-generation AI solutions for their future products.

The product life cycles in many of our target markets typically range from twelve to 24 months. We expect that product lifecycles in the automotive OEM and the industrial and robotics markets will typically be longer than 24 months, as new product introductions occur less frequently. For many of our solutions, early engagement with our customers’ technical staff is necessary for success.

In fiscal year 2025, the customer representing 10% or more of revenue was WT Microelectronics Co., Ltd., formerly Wintech Microelectronics Co., Ltd., or WT, our non-exclusive sales representative and fulfillment partner in Asia other than Japan, which accounted for approximately 63% of total revenue. We currently rely, and expect to continue to rely, on a limited number of customers for a significant portion of our revenue.

Sales and Marketing

We sell our solutions worldwide using our direct sales force and our distributors. We have direct sales personnel covering the United States, Asia and Europe, and we operate sales offices in Santa Clara, California and Hong Kong, and business development offices in China, Germany, Japan, South Korea, and Taiwan. In addition, in each of these locations we employ a staff of field applications engineers to provide direct engineering support locally to our customers.

12

Our sales cycles typically require a significant investment of time and a substantial expenditure of resources before we can realize revenue from the sale of our solutions, if any. Our typical sales cycle consists of a multi-month sales and development process involving our customers’ system designers and management and our sales personnel and software engineers. If successful, this process culminates in a customer’s decision to use our solutions in its system, which we refer to as a design win. Our sales efforts are typically directed to the OEM of the product that will incorporate our AI and video and image processing solution, but the eventual design and incorporation of our SoC into the product may be handled by an ODM or Tier-1 supplier on behalf of the OEM. Volume production may begin within 12 to 18 months after a design win, depending on the complexity of our customer’s product and other factors upon which we may have little or no influence. Once our solutions have been incorporated into a customer’s design, they are likely to be used for the life cycle of the customer’s product. Conversely, a design loss to a competitor will likely preclude any opportunity for future revenue from such customer’s product.

Our sales are generally made pursuant to purchase orders received approximately 20 to 30 weeks prior to the scheduled product delivery date, depending upon agreed terms with our customers and the current manufacturing lead time at the time the purchase order is received. Typically, these purchase orders may not be cancelled or modified without our written consent. Our standard warranty provides that our SoCs containing defects in materials, workmanship or performance may be returned for a refund of the purchase price or for replacement, at our discretion. We may agree to different warranty terms with specific customers from time to time.

Our sales are primarily made through standard purchase orders for delivery of products. Our manufacturing production is based on estimates and advance non-binding commitments from customers as to future purchases. We follow industry practice that generally allows customers to change or defer orders with limited advance notice prior to shipment, often without penalty. Given this practice, we do not believe that backlog is a reliable indicator of future revenue levels, except on a short-term basis, principally within our average lead times.

Manufacturing

We employ a fabless business model and use third-party foundries and assembly and test contractors to manufacture, assemble and test our solutions. This outsourced manufacturing approach allows us to focus our resources on the design, sales and marketing of our solutions and avoid the cost associated with owning and operating our own manufacturing facility. Our engineers work closely with foundries and other contractors to increase yields, lower manufacturing costs and improve quality. In addition, we believe outsourcing many of our manufacturing and assembly activities provides us the flexibility needed to respond to new market opportunities, simplifies our operations and significantly reduces our capital requirements. We do not have a guaranteed level of production capacity from any of our suppliers’ facilities to produce our solutions. We carefully qualify each of our suppliers and their subcontractors and processes in order to meet the extremely high-quality and reliability standards required of our solutions.

Wafer Fabrication

We have a history of using several process nodes from 130 nm through 5 nm. We aim to use the most advanced manufacturing process technology appropriate for our products that is available from our third-party foundries. As a result, we periodically evaluate the benefits of migrating our solutions to smaller geometry process technologies in order to improve performance and efficiency. We believe this strategy will help us remain competitive. We currently manufacture our solutions in the 10nm and 5nm process nodes. Currently, the substantial majority of our SoCs are supplied by Samsung Electronics Corporation (“Samsung”) in facilities located in Austin, Texas and South Korea, from whom we have the option to purchase both fully-assembled and tested products as well as tested die in wafer form for assembly. Our foundry vendors are ISO 9001 certified.

Assembly and Testing

Samsung subcontracts the assembly and initial testing of the assembled chips it supplies to us to Signetics Corporation and STATS ChipPAC Ltd. In the case of purchases of tested die from Samsung, we contract the assembly to Advanced Semiconductor Engineering, Inc., or ASE. Final testing of our products is handled primarily by Sigurd Corporation or King Yuan Electronics Co., Ltd. under the supervision of our engineers. All test software and related processes for our products are developed by our engineers. We continually monitor the results of testing at all of our test contractors to ensure that our testing procedures are properly implemented.

As part of our total quality assurance program, our quality management system has been certified to ISO 9001:2015 standards. Our assembly and testing vendors are also ISO 9001 certified.

Due to the scheduling requirements of our foundry, assembly and test contractors, we generally provide our contractors with our production forecasts and place firm orders for products with our suppliers up to 40 weeks prior to the anticipated delivery date, or potentially longer during times of acute capacity shortages, usually without a purchase order from our own customers.

13

Research and Development

We believe our technology is a competitive advantage and we engage in substantial research and development efforts to develop new products and integrate AI functionality into our video processing solutions. We believe that our continued success depends on our ability to both introduce improved versions of our existing solutions and to develop new solutions for the markets that we serve. As of January 31, 2025, approximately 75% of our employees are engaged in research and development. Our research and development team is comprised of both semiconductor and software designers. Our semiconductor design team has extensive experience in large-scale semiconductor design, including architecture description, logic and circuit design, implementation and verification. Our software design team has extensive experience in development and verification of video processing, AI deep learning and adaptive AI radar software. Because the integration of hardware and software is a key competitive advantage of our solutions, our hardware and software design teams work closely together throughout the product development process. The experience of our hardware and software design teams enables us to effectively assess tradeoffs and advantages when determining which features and capabilities of our solutions should be implemented in hardware and in software.

We have assembled a core team of experienced engineers and systems designers in four research and development design centers located in the United States, China, Italy, and Taiwan.

Competition

The global semiconductor market in general, and the AI and video and image processing markets in particular, are highly competitive. We expect competition to increase and intensify as more and larger semiconductor companies enter our markets and as we penetrate new markets, such as the automotive OEM market. Increased competition could result in price pressure, reduced profitability and loss of market share, any of which could materially and adversely affect our business, revenue and operating results.

Our competitors range from large, international companies offering a wide range of semiconductor products to smaller companies specializing in narrow markets. In the IoT market, our primary competitors include AMLogic Inc., Fuzhou Rockchip Electronics Co., Ltd., HiSilicon Technologies Co., Ltd., or HiSilicon, which is owned by Huawei Technologies Co., Ingenic Semiconductor Co., Ltd.,

Novatek Microelectronics Corp., or Novatek, NVIDIA Corporation, or NVIDIA, OmniVision Technologies, Inc., Qualcomm Incorporated, or Qualcomm, Sigmastar Technology Ltd., and Socionext Inc. In the automotive camera market, we compete against Allwinner Technology Co., Ltd., Horizon Robotics Inc., iCatch Technology, Inc., Mobileye, a subsidiary of Intel Corporation, Novatek, NVIDIA, NXP Semiconductors N.V., Qualcomm, Renesas Electronics Corporation, and Texas Instruments. Certain of our customers and suppliers also have divisions that produce products that compete with ours.

Our ability to compete successfully depends on elements both within and outside of our control, including industry and general economic trends. Many of our competitors are substantially larger, have greater financial, technical, marketing, distribution, customer support and other resources, are more established than we are, and have significantly better brand recognition and broader product offerings which may enable them to develop and enable new technology into product solutions better or faster than us and to better withstand adverse economic or market conditions in the future.

Our ability to compete successfully in the rapidly evolving camera markets depends on several factors, including:

•
the design and manufacturing of new solutions, including software, that anticipate the video processing and integration needs of our customers’ next-generation products and applications; 

•
performance of our AI solutions, as measured by convolutional neural network performance and/or transformer neural network performance, video and still picture image quality, resolution and frame processing rates; 

•
power consumption efficiency of our solutions; 

•
the ease of implementation of our products by customers; 

•
the strength of our customer relationships; 

•
the selection of the foundry process technology and architecture tradeoffs to meet customers’ product requirements in a timely manner; 

•
reputation and reliability; 

•
customer support; and 

•
the cost of the total solution. 

14

We believe that, overall, we compete favorably with respect to these factors, particularly because our solutions typically provide high-quality video, and low power consumption, efficient CNN and transformer

performance, efficient integration of advanced algorithms, exceptional storage and transmission efficiencies, highly-integrated SoC solutions based on a scalable platform, and comprehensive and flexible software. We cannot ensure, however, that our solutions will continue to compete favorably or that we will be successful in the face of increasing competition from new products introduced by existing or new competitors.

Intellectual Property

We rely on a combination of intellectual property rights, including patents, trade secrets, copyrights and trademarks, and contractual protections, to protect our core technology and intellectual property. As of January 31, 2025, we had 372 issued patents in the United States, 137 of which were continuation or divisional patents, 10 issued patents in Europe, 12 issued patents in China, 8 issued patents in Japan and 64 pending patent applications in the United States. The issued patents in the United States expire beginning in 2025 through 2042. Our issued patents and pending patent applications primarily relate to image and video processing and HD video compression, AI processing, system level camera, and radar perception applications spanning multiple market segments
.

We may not receive competitive advantages from any rights granted under our patents, and our patent applications may not result in the issuance of any new patents. In addition, any patent we hold may be opposed, contested, circumvented, designed around by a third party or found to be unenforceable or invalidated. Others may develop technologies that are similar or superior to our proprietary technologies, duplicate our proprietary technologies or design around patents owned or licensed by us.

In addition to our own intellectual property, we also use third-party licenses for certain technologies embedded in our SoC solutions. These are typically non-exclusive contracts provided under royalty-accruing or paid-up licenses. These licenses are generally perpetual or automatically renewed for so long as we continue to pay any maintenance fees that may be due. To date, maintenance fees have not constituted a significant portion of our capital expenditures. While we do not believe our business is dependent to any significant degree on any individual third-party license, we expect to continue to use and may license additional third-party technology for our solutions.

We generally control access to and use of our confidential information through employing internal and external controls, including contractual protections with employees, contractors and customers. We rely in part on U.S. and international copyright laws to protect our mask work. All employees and consultants are required to execute confidentiality agreements in connection with their employment and consulting relationships with us. We also require them to agree to disclose and assign to us all inventions conceived or made in connection with the employment or consulting relationship.

Despite our efforts to protect our intellectual property, unauthorized parties may still copy or otherwise obtain and use our software, technology or other information that we regard as proprietary intellectual property. In addition, we continue to operate internationally, and effective patent, copyright, trademark and trade secret protection may not be available or may be limited in foreign countries.

Seasonality

Our business has tended to be seasonal with higher revenue in our second and third fiscal quarters as our customers typically increase their production to meet holiday shopping season or year-end demand for their products. We also may experience seasonally lower demand in our first and fourth fiscal quarters due to ODM and OEM factory holiday closures. These seasonal fluctuations may diminish if our revenue diversifies and becomes less dependent on sales of our customers’ consumer products.

Governmental Regulation

Our business and operations around the world are subject to government regulation at the national, state or local level addressing, among other matters, applicable environmental laws, health and safety laws and regulations, laws relating to export controls and economic sanctions, and the rules of industrial standards bodies such as the International Standards Organization and governmental agencies such as the Federal Trade Commission.

We believe that our properties and operations comply in all material respects with applicable laws protecting the environment and worker health and safety. As a fabless semiconductor company, we do not manufacture our own products but do maintain laboratory space at certain of our facilities to facilitate the development, evaluation and testing of our SoC products. To date, we have not incurred significant expenditures relating to environmental compliance at our facilities nor have we experienced any material issues relating to employee health and safety.

15

In addition to environmental and worker health and safety laws, our business is subject to various rules and regulations and executive orders relating to export controls and trade sanctions. Certain of our products are subject to the Export Administration Regulations (EAR), which are administered by the United States Department of Commerce’s Bureau of Industry and Security (BIS), and we may from time to time be required to obtain an export license before we can export certain products or technology to specified countries or customers. In addition, the EAR imposes broad controls on entities listed on sanctioned persons lists, including the BIS Entity List. If one of our customers is listed on the BIS Entity List or another U.S. government sanctioned persons’ list, then subject to certain exceptions, we will, as a general rule, be precluded from doing business with that customer. We cannot guarantee that export control restrictions or sanctions imposed in the future will not prevent, or materially limit, our ability to conduct business with certain customers or in certain countries. Any failure to comply with these laws could result in governmental enforcement actions, including substantial monetary penalties and denial of export privileges.

Human Capital Resources

Innovation has been the lifeblood of our company since our founding in 2004.  We continually strive to develop leading-edge image and video, and now AI, processors using the most advanced semiconductor processes available to create high performance, power efficient SoCs. We depend on our people to sustain our competitive advantages.

As of January 31, 2025, we employed a total of 941 people, including 260 in the United States, 590 in Asia, primarily with 352 in Taiwan and 223 in China, and 91 in Europe. Approximately 75% of our employees are engaged in research and development, 23% in sales, marketing and administration and 2% in operations. As of January 31, 2025, women represented 33% of our independent directors, 18% of senior management, 17% of our technical roles, and 20% of our total workforce. Of our total employee workforce, approximately 37% is represented by a work council in Taiwan. The work council group, which is common in Taiwan, is comprised of employees elected by the general employee base in that location. We consider our global employee relations to be good. Despite employees working in geographically disparate locations and differences in cultures, we strive to treat all employees as part of one team working together. Our Chief Executive Officer holds quarterly town hall style meetings with employees of all of our offices to keep employees apprised of company activities and objectives and to provide an opportunity for all employees to meet and ask questions. All employees receive training in the prevention of sexual harassment and abusive conduct in the workplace.

Our human capital resources objectives include attracting and retaining talented and experienced employees. We utilize multiple online search tools, specialized recruiting firms, employee referral programs and university hires to ensure a varied outreach approach for candidates. We are committed to ensuring the human rights of our worldwide workforce and treating all employees with dignity and respect. We offer a combination of competitive base salary, time-based equity incentives and bonus plans linked to financial and strategic performance that are designed to motivate and reward personnel with annual grants of stock-based and cash-based incentive compensation awards, plus other benefits, in order to increase both stockholder value and the success of our company by motivating such individuals to perform to the best of their abilities and achieve our short term and long-term objectives. We offer competitive benefits tailored to local markets and laws and designed to support employee health, welfare and retirement; examples of such benefits include paid time off; 401(k), pension or other retirement plans; an employee stock purchase plan; basic and voluntary life, disability and supplemental insurance; medical, dental and vision insurance; health savings and flexible spending accounts; relocation assistance; and employee assistance programs.  Approximately 90% of eligible U.S. employees participate in our 401(k) plan, and 90% of eligible employees participated in the most recent offering period of our employee stock purchase plan.

The average tenure of our employees is approximately 8.1 years and approximately 33% of our employees have been employed by us for more than 10 years. We believe our compensation and benefits packages, combined with our culture that promotes teamwork, innovation and hands-on experience from the first day of employment, contribute to low employee turnover and an above-average tenure. We monitor employee turnover rates by region and our company as a whole. Our worldwide voluntary employee turnover rate in fiscal year 2025 was approximately 3.3%.

16

Corporate Information

Ambarella was founded and incorporated in the Cayman Islands in January 2004. Our registered address is PO Box 309GT, Ugland House, South Church Street, George Town, Grand Cayman, Cayman Islands. The address of our U.S. operating subsidiary is Ambarella Corporation, 3101 Jay Street, Santa Clara, California. The Securities and Exchange Commission, or SEC, maintains a website at 
www.sec.gov 
that contains reports, proxy, and information statements, and other information regarding registrants that file electronically. You may also obtain copies of our Forms 10-K, 10-Q, 8-K, and other filings with the SEC, and all amendments to these filings, free of charge, by visiting the Investor Relations page on our website (
http://investor.ambarella.com
)
 as soon as reasonably practicable following our filing of any of these reports with the SEC. Information on our website is not incorporated into this Annual Report on Form 10-K or our other securities filings and is not a part of such filings.